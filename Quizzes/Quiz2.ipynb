{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quiz2.ipynb","provenance":[{"file_id":"1yaxzhHf78x1FkTrbOomGvPACK8w5vhVw","timestamp":1615335473162},{"file_id":"1G-LbqlHQCecxfb-v_UhNEOTT7HBC3O5e","timestamp":1614796521339},{"file_id":"12hosWLJGK2lXnWxqY6p-XnvDGWXSBSFj","timestamp":1614718948511},{"file_id":"1KxVcH2lzczYuqYZOgxTvi5tRlci84Z9k","timestamp":1614129250705},{"file_id":"1PBGp1ZAUAAzDKvlF2gb2BWW9wZS9la5W","timestamp":1614021663800}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uCqdefS54-hN"},"source":["# CS530 Quiz 2"]},{"cell_type":"code","metadata":{"id":"VJ-fTs0qCHZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615335515742,"user_tz":480,"elapsed":2248,"user":{"displayName":"Toby Chappell","photoUrl":"","userId":"06079717396424678337"}},"outputId":"ae49687d-0162-434a-daed-256e29f54f93"},"source":["# make datasets\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import statsmodels.api as sm\r\n","from sklearn.model_selection import train_test_split\r\n","import sklearn.linear_model as lm\r\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"abcCGF6qREFM"},"source":["The first module is devoted to regression and regularization, which we learned in the context of linear regression. Although this technique is ubiquitous in machine learning and deep learning. The main idea behind regularization, as we discussed in class, is that—when successful—it greatly decrease the variance of a model for the price of just a small increase to the bias: \r\n","$$ MSE = [Bias(\\hat{f}(x_0))]^2 + Var(\\hat{f}(x_0)) + Var(\\epsilon) $$ Or\r\n","$$ Err(x) = (E[\\hat{f}(x)]-f(x))^2 + E[(\\hat{f}(x)-E[\\hat{f}(x)])^2] + \\sigma_{\\epsilon}^2 $$\r\n","\r\n","From the above it is clear that if the variance decreases more than the bias increases, the overall error on the test set (the MSE) decreases, which is what we want. \r\n","\r\n","1. Read the code below, which describes part of a simulation in the context of the bias-variance discussion above.\r\n","\r\n","  (a) Explain what each of the following variables represent in the code: \"X\", \"f_x\", \"eps\", and \"y\".\r\n","\r\n","  (b) For the dataset described by this code, would you expect the variance to be high or low? Explain your answer. \r\n"]},{"cell_type":"code","metadata":{"id":"V9FjWI5ryrkL","executionInfo":{"status":"ok","timestamp":1615335546936,"user_tz":480,"elapsed":350,"user":{"displayName":"Toby Chappell","photoUrl":"","userId":"06079717396424678337"}}},"source":["np.random.seed(42)\r\n","n_informative = 25\r\n","x_dims = [1000, n_informative]\r\n","beta_0 = 10\r\n","eps_std = 20;\r\n","\r\n","X = np.random.normal(size=x_dims)\r\n","for k in range(0, 12):\r\n","    noise = np.random.normal(0,0.1,size=(x_dims[0],1)).reshape(-1)\r\n","    X[:,-(k+1)]=X[:,k] + noise\r\n","f_x = np.sum(100*X[:,:n_informative], axis=1) + beta_0\r\n","eps = np.random.normal(0,eps_std,size=(x_dims[0],1)).reshape(-1)\r\n","y = f_x + eps\r\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrJJok390JDe"},"source":["a) \n","\n","X: This the data drawn from a random normal sample. It has 25 features and 1000 rows. In addition, there is some added noise to X shown in the for loop.\n","\n","f_x: This is the true model of the data, found by summing X*100 and then adding 10. This is the model we are trying to achieve.\n","\n","eps: This is irreducible error drawn from a random normal sample with a standard deviation of 20. It is purely random and does not play factor into the true model (f_x does not rely on eps).\n","\n","y: These would be the y labels given in the dataset, found by the summation of the true model and some irrecuible error.\n","\n","b)\n","\n","I would expect the variance to be fairly high since there is a considerable amount of noise brought into the model."]},{"cell_type":"markdown","metadata":{"id":"MI8oQ-nM0M3V"},"source":["2. Say that we fit a ridge regression model on this data set, as we learned in class.\n","\n","  (a) What would happen to the coefficients of the model (i.e., the $\\hat \\beta$') as you increase the value of the regularization parameter, $\\alpha$ (or, in the class slides $\\lambda$)? \n","  \n","  (b) How do you expect that bias and variance of the fitted model would change as $\\alpha$ increases? And how would the MSE change? Explain the reasoning behind your answers."]},{"cell_type":"markdown","metadata":{"id":"nPMWO-n_07iV"},"source":["a)\n","\n","The coefficients would approach 0 as alpha increases since the model would become increasingly regularized.\n","\n","b)\n","\n","The bias will increase while the variance will decrease as alpha increases (point of regularization is to increase bias slightly in order to decrease variance signifcantly). The MSE as such would decrease for awhile and then increase. The reason for this is because, since a small increase in bias leads to a large decrease in variance, the MSE will decrease for awhile. However, once bias is too large, it will begin to increase again."]},{"cell_type":"markdown","metadata":{"id":"HXJwUK1x1cls"},"source":["3. After simulating 1000 these datasets, you are able to estimate the bias and variance for ridge regression models on each of the selected $\\alpha$'s. When you plot the variance and bias over the $\\alpha$ paramter, you get the following output:\r\n","\r\n","![images.png](data:image/png;base64,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)\r\n","\r\n","Explain which part of the plot shows the bias, variance, MSE and irreducible error.\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"W8Ei3w0v3jgK"},"source":["The MSE is the U shaped line (decreases and then increases). The bias is the dotted line that is increasing as alpha increases. The variance is the dotted line that is decreasing as alpha increases. The irreducible error should be a constant line somewhere below the minimum of the MSE (irreducible error <= minimum MSE) "]},{"cell_type":"markdown","metadata":{"id":"F9c3rTf53lnn"},"source":["4. We learned about using cross-validation to find the best $\\alpha$ parameter for ridge regression in our class. Can you also use this bias and variance trade-off method to find the best $\\alpha$ parameter for real-life datasets? Explain."]},{"cell_type":"markdown","metadata":{"id":"vUQ2FTxC4x9b"},"source":["You could use this method on real life data, but there are certain factors that need to be taken into consideration. For one, if your dataset is large enough or if your model is already simple, regularization is probably not needed. However, if you have a large number of parameters and a complex model regularization would be appropriate. The reason for this is because regularization would help prevent against overfitting and would reduce the number of parameters (assuming the alpha is large enough). Using cross validation to do this would help strengthen the accuracy of the optimal alpha to use."]}]}